# Evaluation
We provide details on the evaluation of the models in this directory. Including ranking accuracy, alignment performance and downstream task evaluation.

## Ranking Accuracy
Specifically, we evaluate ranking accuracy on [RM-Bench](https://github.com/THU-KEG/RM-Bench) and [rm-robustness](https://huggingface.co/rm-robustness). RM-Bench is a 
challenging benchmark designed to test a model’s grasp of subtle preference nuances. RM-Robustness proposed four generalization scenarios on the Ul-traFeedBack: 
In-Distribution (ID), Prompt-OOD, Response-OOD, and Mutual-OOD to evaluate the robustness of Reward Model.

### RM-Bench
We provide experiment setups and examples of RM-Bench as follows:
We use `rewardbench==0.1.3` and `vllm==0.7.1` for general experiment setups.

To evaluate the ranking accuracy of trained policy model, we test under **no reference model mode** in RM-Bench, we provide the scripts in `RM-Bench/run_no_ref.sh` to replace the 
`run_dpo.sh` in RM-Bench, which leverages the **average log probs** to measure ranking accuracy. 

Please don't forget to change the `chat_template` in `RM-Bench/run_no_ref.sh` according to the model.

### RM-Robustness
We train DPO, SimPO and AMaPo with **the same setting as other experiments**, which you can find in appendix and `environment.yml`, only changing the training dataset and the test protocol. 

We implement DPO and SimPO with `alignment-handbook`. The ranking accuracy test protocol via average log probs can be found in `scripts/AMaPO_trainer`.

## Alignment Performance
Specifically, we evaluate alignment performance on AlpacaEval 2 and MT-Bench. AlpacaEval 2 consists of 805 questions from 5 datasets, and MT-Bench covers 8 categories with 80 questions. We report scores following each
benchmark’s evaluation protocol. For AlpacaEval 2, we report both the raw win rate (WR) and the
length-controlled win rate (LC). The LC metric is specifically designed to be robust against model verbosity. For MT-Bench, we report the GPT-4 Turbo Score in average of two run. 

### AlpacaEval 2
We provide generation configurations for the released models in the `alpacaeval2/configs` directory, and the corresponding generation templates can be found in `alpacaeval2/templates`. To evaluate the models on AlpacaEval 2, please use the [`alpaca-eval`](https://github.com/tatsu-lab/alpaca_eval) package.
Example evaluation scripts are as follows:

```
OPENAI_CLIENT_CONFIG_PATH=/eval/alpacaeval2/configs/openai_configs.yaml \
VLLM_WORKER_MULTIPROC_METHOD=spawn \
alpaca_eval evaluate_from_model \
--model_configs '/eval/alpacaeval2/configs/Llama-3-Base-8B-AMaPO.yaml' \
--evaluation_dataset '/eval/alpacaeval2/data/alpaca_eval_gpt4_baseline.json' \
--annotators_config 'weighted_alpaca_eval_gpt4_turbo'
```

### MT-Bench
We provide the reference answers generated by GPT-4 Turbo in the `mt-bench` directory which are more accurate than the original GPT-4 generated answers. To evaluate the models on MT-Bench, please use the [`FastChat LLM Judge`](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#mt-bench) package and replace the [reference answers generated by GPT-4](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/mt_bench/reference_answer/gpt-4.jsonl). 
We use `fschat==0.2.36` for MT-Bench evaluation including answer generation and GPT-4 Turbo judgement

## Downsteam Task
We evaluate the downstream tasks in [Open LLM Leaderboard](https://huggingface.co/open-llm-leaderboard) with [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), 
where `lm_eval==0.4.5` and `vllm==0.8.0` following [SimPER](https://github.com/tengxiao1/SimPER). 

We provide a sample script to evaluate the downstream tasks with `lm_eval` in `open_llm_leaderboard/eval.py`